{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings as wr\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "wr.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_lbl = r\"D:\\Documentos\\Facu\\Organizacion de Datos\\TP2\\Labels\"\n",
    "loc_ftr = r\"D:\\Documentos\\Facu\\Organizacion de Datos\\TP2\\Features\\FeaturesLocalST\"\n",
    "\n",
    "label_trn = pd.read_csv( loc_lbl + \"\\\\label_auc_21_23.csv\" )\n",
    "label_tst = pd.read_csv( loc_lbl + \"\\\\label_auc_24_26.csv\" )\n",
    "\n",
    "X = label_trn[ [\"ref_hash\"] ]; Z = label_tst[ [\"ref_hash\"] ]\n",
    "Y = label_trn[ [\"ref_hash\",\"21_23_st\"] ]; W = label_tst[ [\"ref_hash\",\"24_26_st\"] ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">  Preparo los datos para predecir </span>\n",
    "Con los datos 21-23 predecimos \"24-26_sc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_var = Z; W_var = W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregamos los features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_01_Z = pd.read_csv( loc_ftr + \"\\\\auc_aftr_tst.csv\" ); Z_var = Z_var.merge( ftr_01_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_02_Z = pd.read_csv( loc_ftr + \"\\\\auc_mday_tst.csv\" ); Z_var = Z_var.merge( ftr_02_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_03_Z = pd.read_csv( loc_ftr + \"\\\\auc_morn_tst.csv\" ); Z_var = Z_var.merge( ftr_03_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_04_Z = pd.read_csv( loc_ftr + \"\\\\auc_nght_tst.csv\" ); Z_var = Z_var.merge( ftr_04_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_05_Z = pd.read_csv( loc_ftr + \"\\\\cant_auc_tst.csv\" ); Z_var = Z_var.merge( ftr_05_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_06_Z = pd.read_csv( loc_ftr + \"\\\\cant_clk_tst.csv\" ); Z_var = Z_var.merge( ftr_06_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_07_Z = pd.read_csv( loc_ftr + \"\\\\cant_evt_tst.csv\" ); Z_var = Z_var.merge( ftr_07_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_08_Z = pd.read_csv( loc_ftr + \"\\\\cevt_atr_tst.csv\" ); Z_var = Z_var.merge( ftr_08_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_09_Z = pd.read_csv( loc_ftr + \"\\\\cins_atr_tst.csv\" ); Z_var = Z_var.merge( ftr_09_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_10_Z = pd.read_csv( loc_ftr + \"\\\\cins_imp_tst.csv\" ); Z_var = Z_var.merge( ftr_10_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_11_Z = pd.read_csv( loc_ftr + \"\\\\clk_aftr_tst.csv\" ); Z_var = Z_var.merge( ftr_11_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_12_Z = pd.read_csv( loc_ftr + \"\\\\clk_mday_tst.csv\" ); Z_var = Z_var.merge( ftr_12_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_13_Z = pd.read_csv( loc_ftr + \"\\\\clk_morn_tst.csv\" ); Z_var = Z_var.merge( ftr_13_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_14_Z = pd.read_csv( loc_ftr + \"\\\\clk_ngth_tst.csv\" ); Z_var = Z_var.merge( ftr_14_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_15_Z = pd.read_csv( loc_ftr + \"\\\\event_id_tst.csv\" ); Z_var = Z_var.merge( ftr_15_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_16_Z = pd.read_csv( loc_ftr + \"\\\\evt_aftr_tst.csv\" ); Z_var = Z_var.merge( ftr_16_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_17_Z = pd.read_csv( loc_ftr + \"\\\\evt_mday_tst.csv\" ); Z_var = Z_var.merge( ftr_17_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_18_Z = pd.read_csv( loc_ftr + \"\\\\evt_morn_tst.csv\" ); Z_var = Z_var.merge( ftr_18_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_19_Z = pd.read_csv( loc_ftr + \"\\\\evt_nght_tst.csv\" ); Z_var = Z_var.merge( ftr_19_Z, how = \"inner\", on = \"ref_hash\" )\n",
    "ftr_20_Z = pd.read_csv( loc_ftr + \"\\\\frst_auc_tst.csv\" ); Z_var = Z_var.merge( ftr_20_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_21_Z = pd.read_csv( loc_ftr + \"\\\\frst_clk_tst.csv\" ); Z_var = Z_var.merge( ftr_21_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_22_Z = pd.read_csv( loc_ftr + \"\\\\frst_evt_tst.csv\" ); Z_var = Z_var.merge( ftr_22_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_23_Z = pd.read_csv( loc_ftr + \"\\\\frst_ins_tst.csv\" ); Z_var = Z_var.merge( ftr_23_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_24_Z = pd.read_csv( loc_ftr + \"\\\\hora_auc_tst.csv\" ); Z_var = Z_var.merge( ftr_24_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_25_Z = pd.read_csv( loc_ftr + \"\\\\hr_f_evt_tst.csv\" ); Z_var = Z_var.merge( ftr_25_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_26_Z = pd.read_csv( loc_ftr + \"\\\\ins_afme_tst.csv\" ); Z_var = Z_var.merge( ftr_26_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_27_Z = pd.read_csv( loc_ftr + \"\\\\ins_mdoh_tst.csv\" ); Z_var = Z_var.merge( ftr_27_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_28_Z = pd.read_csv( loc_ftr + \"\\\\ins_mroh_tst.csv\" ); Z_var = Z_var.merge( ftr_28_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_29_Z = pd.read_csv( loc_ftr + \"\\\\ins_ngoh_tst.csv\" ); Z_var = Z_var.merge( ftr_29_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_30_Z = pd.read_csv( loc_ftr + \"\\\\kind_evt_tst.csv\" ); Z_var = Z_var.merge( ftr_30_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_31_Z = pd.read_csv( loc_ftr + \"\\\\last_auc_tst.csv\" ); Z_var = Z_var.merge( ftr_31_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_32_Z = pd.read_csv( loc_ftr + \"\\\\last_evt_tst.csv\" ); Z_var = Z_var.merge( ftr_32_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_33_Z = pd.read_csv( loc_ftr + \"\\\\last_ins_tst.csv\" ); Z_var = Z_var.merge( ftr_33_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_34_Z = pd.read_csv( loc_ftr + \"\\\\m1oh_auc_tst.csv\" ); Z_var = Z_var.merge( ftr_34_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_35_Z = pd.read_csv( loc_ftr + \"\\\\main_ahr_tst.csv\" ); Z_var = Z_var.merge( ftr_35_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_36_Z = pd.read_csv( loc_ftr + \"\\\\main_ehr_tst.csv\" ); Z_var = Z_var.merge( ftr_36_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_37_Z = pd.read_csv( loc_ftr + \"\\\\ref_type_tst.csv\" ); Z_var = Z_var.merge( ftr_37_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_38_Z = pd.read_csv( loc_ftr + \"\\\\rh_encod_tst.csv\" ); Z_var = Z_var.merge( ftr_38_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_39_Z = pd.read_csv( loc_ftr + \"\\\\sdia_auc_tst.csv\" ); Z_var = Z_var.merge( ftr_39_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_40_Z = pd.read_csv( loc_ftr + \"\\\\srce_auc_tst.csv\" ); Z_var = Z_var.merge( ftr_40_Z, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_41_Z = pd.read_csv( loc_ftr + \"\\\\wifi_evt_tst.csv\" ); Z_var = Z_var.merge( ftr_41_Z, how = \"inner\", on = \"ref_hash\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">  Preparo los datos para entrenar </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos 18-20 + \"21-23_sc\" entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_var = Y; X_var = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregamos los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_01_X = pd.read_csv( loc_ftr + \"\\\\auc_aftr_trn.csv\" ); X_var = X_var.merge( ftr_01_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_02_X = pd.read_csv( loc_ftr + \"\\\\auc_mday_trn.csv\" ); X_var = X_var.merge( ftr_02_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_03_X = pd.read_csv( loc_ftr + \"\\\\auc_morn_trn.csv\" ); X_var = X_var.merge( ftr_03_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_04_X = pd.read_csv( loc_ftr + \"\\\\auc_nght_trn.csv\" ); X_var = X_var.merge( ftr_04_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_05_X = pd.read_csv( loc_ftr + \"\\\\cant_auc_trn.csv\" ); X_var = X_var.merge( ftr_05_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_06_X = pd.read_csv( loc_ftr + \"\\\\cant_clk_trn.csv\" ); X_var = X_var.merge( ftr_06_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_07_X = pd.read_csv( loc_ftr + \"\\\\cant_evt_trn.csv\" ); X_var = X_var.merge( ftr_07_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_08_X = pd.read_csv( loc_ftr + \"\\\\cevt_atr_trn.csv\" ); X_var = X_var.merge( ftr_08_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_09_X = pd.read_csv( loc_ftr + \"\\\\cins_atr_trn.csv\" ); X_var = X_var.merge( ftr_09_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_10_X = pd.read_csv( loc_ftr + \"\\\\cins_imp_trn.csv\" ); X_var = X_var.merge( ftr_10_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_11_X = pd.read_csv( loc_ftr + \"\\\\clk_aftr_trn.csv\" ); X_var = X_var.merge( ftr_11_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_12_X = pd.read_csv( loc_ftr + \"\\\\clk_mday_trn.csv\" ); X_var = X_var.merge( ftr_12_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_13_X = pd.read_csv( loc_ftr + \"\\\\clk_morn_trn.csv\" ); X_var = X_var.merge( ftr_13_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_14_X = pd.read_csv( loc_ftr + \"\\\\clk_ngth_trn.csv\" ); X_var = X_var.merge( ftr_14_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_15_X = pd.read_csv( loc_ftr + \"\\\\event_id_trn.csv\" ); X_var = X_var.merge( ftr_15_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_16_X = pd.read_csv( loc_ftr + \"\\\\evt_aftr_trn.csv\" ); X_var = X_var.merge( ftr_16_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_17_X = pd.read_csv( loc_ftr + \"\\\\evt_mday_trn.csv\" ); X_var = X_var.merge( ftr_17_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_18_X = pd.read_csv( loc_ftr + \"\\\\evt_morn_trn.csv\" ); X_var = X_var.merge( ftr_18_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_19_X = pd.read_csv( loc_ftr + \"\\\\evt_nght_trn.csv\" ); X_var = X_var.merge( ftr_19_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_20_X = pd.read_csv( loc_ftr + \"\\\\frst_auc_trn.csv\" ); X_var = X_var.merge( ftr_20_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_21_X = pd.read_csv( loc_ftr + \"\\\\frst_clk_trn.csv\" ); X_var = X_var.merge( ftr_21_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_22_X = pd.read_csv( loc_ftr + \"\\\\frst_evt_trn.csv\" ); X_var = X_var.merge( ftr_22_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_23_X = pd.read_csv( loc_ftr + \"\\\\frst_ins_trn.csv\" ); X_var = X_var.merge( ftr_23_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_24_X = pd.read_csv( loc_ftr + \"\\\\hora_auc_trn.csv\" ); X_var = X_var.merge( ftr_24_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_25_X = pd.read_csv( loc_ftr + \"\\\\hr_f_evt_trn.csv\" ); X_var = X_var.merge( ftr_25_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_26_X = pd.read_csv( loc_ftr + \"\\\\ins_afme_trn.csv\" ); X_var = X_var.merge( ftr_26_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_27_X = pd.read_csv( loc_ftr + \"\\\\ins_mdoh_trn.csv\" ); X_var = X_var.merge( ftr_27_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_28_X = pd.read_csv( loc_ftr + \"\\\\ins_mroh_trn.csv\" ); X_var = X_var.merge( ftr_28_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_29_X = pd.read_csv( loc_ftr + \"\\\\ins_ngoh_trn.csv\" ); X_var = X_var.merge( ftr_29_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_30_X = pd.read_csv( loc_ftr + \"\\\\kind_evt_trn.csv\" ); X_var = X_var.merge( ftr_30_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_31_X = pd.read_csv( loc_ftr + \"\\\\last_auc_trn.csv\" ); X_var = X_var.merge( ftr_31_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_32_X = pd.read_csv( loc_ftr + \"\\\\last_evt_trn.csv\" ); X_var = X_var.merge( ftr_32_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_33_X = pd.read_csv( loc_ftr + \"\\\\last_ins_trn.csv\" ); X_var = X_var.merge( ftr_33_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_34_X = pd.read_csv( loc_ftr + \"\\\\m1oh_auc_trn.csv\" ); X_var = X_var.merge( ftr_34_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_35_X = pd.read_csv( loc_ftr + \"\\\\main_ahr_trn.csv\" ); X_var = X_var.merge( ftr_35_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_36_X = pd.read_csv( loc_ftr + \"\\\\main_ehr_trn.csv\" ); X_var = X_var.merge( ftr_36_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_37_X = pd.read_csv( loc_ftr + \"\\\\ref_type_trn.csv\" ); X_var = X_var.merge( ftr_37_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_38_X = pd.read_csv( loc_ftr + \"\\\\rh_encod_trn.csv\" ); X_var = X_var.merge( ftr_38_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_39_X = pd.read_csv( loc_ftr + \"\\\\sdia_auc_trn.csv\" ); X_var = X_var.merge( ftr_39_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_40_X = pd.read_csv( loc_ftr + \"\\\\srce_auc_trn.csv\" ); X_var = X_var.merge( ftr_40_X, how = \"inner\", on = \"ref_hash\" ) \n",
    "ftr_41_X = pd.read_csv( loc_ftr + \"\\\\wifi_evt_trn.csv\" ); X_var = X_var.merge( ftr_41_X, how = \"inner\", on = \"ref_hash\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_var = X_var.merge( Y_var, how = \"inner\", on = \"ref_hash\" ); \n",
    "Z_var = Z_var.merge( W_var, how = \"inner\", on = \"ref_hash\" ); \n",
    "\n",
    "Y_var = X_var[ [\"21_23_st\"] ]; X_var = X_var.drop( [\"ref_hash\", \"21_23_st\"], axis = 1 )\n",
    "W_var = Z_var[ [\"24_26_st\"] ]; Z_var = Z_var.drop( [\"ref_hash\", \"24_26_st\"], axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X_var.columns:\n",
    "    X_var[column] = X_var[column].fillna((X_var[column].mean()))\n",
    "\n",
    "for column in Z_var.columns:\n",
    "    Z_var[column] = Z_var[column].fillna((Z_var[column].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediccion con <span style=\"color:green\"> *SupportVectorMachine*</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 89498.762500\n"
     ]
    }
   ],
   "source": [
    "estimator = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "    gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "fit = estimator.fit( X_var,Y_var )\n",
    "prd = estimator.predict( Z_var)\n",
    "rmse = np.sqrt( mean_squared_error(W_var, prd) )\n",
    "print(\"RMSE: %f\" % (rmse) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probamos features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "thresholds = sort(fit.feature_importances_)\n",
    "i = 0; pred = []; mod_sel = [];\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(fit, threshold=thresh, prefit=True)\n",
    "    select_X = selection.transform(X_var)\n",
    "    # train model\n",
    "    selection_model = xgb.XGBRegressor( **prm )\n",
    "    mod_sel.append( selection_model.fit(select_X, Y_var) )\n",
    "    # eval model\n",
    "    select_Z = selection.transform(Z_var)\n",
    "    pred.append( selection_model.predict(select_Z) )\n",
    "    rmse = np.sqrt( mean_squared_error(pred[i], W_var) )\n",
    "    print(\"Thresh=%.3f, n=%d, RMSE:%f, model=%d\" % (thresh, select_X.shape[1], rmse, i) )\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning de HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "mse = make_scorer( mean_squared_error, greater_is_better = False )\n",
    "\n",
    "reg = AdaBoostRegressor()\n",
    "\n",
    "tune_prms = {'base_estimator':[None], \n",
    "             'learning_rate':[1.5], \n",
    "             'loss':['linear', 'square','exponential'],\n",
    "             'n_estimators':[25,50,100,150,250,500],\n",
    "             'random_state':[1,5,10,15,20,30,40,50,75,100]\n",
    "            }\n",
    "\n",
    "grid = RandomizedSearchCV( estimator = reg, param_distributions = tune_prms, cv = 10, n_jobs = 1, scoring = mse, n_iter = 20 ) \n",
    "grid.fit( X_var, Y_var )    \n",
    "\n",
    "# Results from Grid Search\n",
    "print(\"==========================================================\")\n",
    "print(\"||            Results from Grid Search                  ||\")\n",
    "print(\"==========================================================\")    \n",
    "    \n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", grid.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", grid.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "mse = make_scorer( mean_squared_error, greater_is_better = False )\n",
    "\n",
    "reg = GradientBoostingRegressor()\n",
    "\n",
    "tune_prms = {\n",
    "    'loss' : ['ls', 'lad', 'huber', 'quantile'],\n",
    "    'learning_rate' : [0.1,0.2,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "    'n_estimators':[25,50,100,150,250,500],\n",
    "    'subsample' : [0.1,0.3,0.6,0.7,0.9],\n",
    "    'criterion' : ['friedman_mse'],\n",
    "    'min_samples_split' :[2,4,6,10],\n",
    "    'min_weight_fraction_leaf' : [0.1,0.2,0.0,0.5],\n",
    "    'max_depth' :[3,4,5,7,10],\n",
    "    'random_state' : [10,25,50,100],\n",
    "    'max_features' : [10,20]\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV( estimator = reg, param_distributions = tune_prms, cv = 10, n_jobs = 1, scoring = mse, n_iter = 20 ) \n",
    "grid.fit( X_var, Y_var )    \n",
    "\n",
    "# Results from Grid Search\n",
    "print(\"==========================================================\")\n",
    "print(\"||            Results from Grid Search                  ||\")\n",
    "print(\"==========================================================\")    \n",
    "    \n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", grid.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", grid.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
